{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJR5cB3fdgXL"
      },
      "source": [
        "# Initializing Notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABuvVAxEdtkS"
      },
      "source": [
        "## Installing Dependencies \n",
        "\n",
        "❗❗Restart runtime after installing dependencies and before importing libraries to use updated libraries\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install tensorflow_datasets \n",
        "%pip install tf-models-official\n",
        "%pip install transformers\n",
        "%pip install tensorflow\n",
        "%pip install scikit-learn\n",
        "%pip install seaborn\n",
        "%pip install emoji\n",
        "%pip install contractions\n",
        "%pip install sentencepiece\n",
        "%pip install nltk\n",
        "%pip install matplotlib\n",
        "%pip install wordcloud\n",
        "%pip install plotly\n",
        "%pip install tqdm\n"
      ],
      "metadata": {
        "id": "LsR7RmhDzsHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRKAvaFGdyOZ"
      },
      "source": [
        "## Importing Libraries & Setting Seed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sk7Z4y25tQTG"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import seaborn\n",
        "import sklearn\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "from functools import partial\n",
        "import PIL\n",
        "import PIL.Image\n",
        "import pandas as pd\n",
        "\n",
        "# %tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "from keras import backend as K\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "from collections import Counter\n",
        "import collections\n",
        "import re\n",
        "import unicodedata\n",
        "import emoji\n",
        "import contractions\n",
        "import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKYTeYVPeOSg"
      },
      "source": [
        "### Setting Global Seed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dmsmDNVXeKbG"
      },
      "outputs": [],
      "source": [
        " # note that you must use the same seed to ensure consistentcy in your training/validation/testing\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2gcGS21E04-8"
      },
      "outputs": [],
      "source": [
        "#Suppress 'SettingWithCopyWarning' from pandas \n",
        "pd.options.mode.chained_assignment = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nSvOy40eXKO"
      },
      "source": [
        "# Data Handling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxHuFuEEefgn"
      },
      "source": [
        "### Loading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-5katbFWO1lz"
      },
      "outputs": [],
      "source": [
        "ds = tfds.load('goemotions')\n",
        "#look up how it can split it splits easily"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_7zDT_hmQXz"
      },
      "source": [
        "### Cleaning the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We remove all the data which has multiple labels assigned. "
      ],
      "metadata": {
        "id": "G8ia-RXYYqAr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a6-DK-I11F9A"
      },
      "outputs": [],
      "source": [
        "def remove_multilabels(ds, split):\n",
        "  '''\n",
        "  Removes items with multiple labels\n",
        "  '''\n",
        "  df = tfds.as_dataframe(ds[split])\n",
        "  df = df[df[df.columns.difference(['comment_text'])].sum(axis=1) == 1]\n",
        "  return df\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_columns(df, split):\n",
        "  df.loc[:, 'love'] += df['caring']\n",
        "  df.loc[:, 'approval'] += df['admiration']\n",
        "  df.loc[:, 'disapproval'] += df['disgust'] + df['anger'] + df['annoyance']\n",
        "  df.loc[:, 'surprise'] += df['curiosity']\n",
        "  df.loc[:, 'fear'] += df['nervousness']\n",
        "  df.loc[:, 'sadness'] += df['embarrassment'] + df['grief'] + df['remorse'] + df['disappointment']\n",
        "  df.loc[:, 'joy'] += df['amusement'] + df['excitement'] + df['relief']\n",
        "  df.drop(['caring', 'admiration', 'disgust', 'anger', 'annoyance', 'curiosity', 'nervousness', 'embarrassment',\n",
        "            'grief', 'remorse', 'disappointment', 'relief', 'excitement', 'amusement'], axis=1, inplace=True)\n",
        "  return df"
      ],
      "metadata": {
        "id": "OwYgWif0Y7ux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IOgMYGDY36o8"
      },
      "outputs": [],
      "source": [
        "ds_train = merge_columns(remove_multilabels(ds, 'train'), 'train')\n",
        "ds_valid = merge_columns(remove_multilabels(ds, 'validation'), 'validation')\n",
        "ds_test = merge_columns(remove_multilabels(ds, 'test'), 'test')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-LW5TrAS6zu"
      },
      "source": [
        "Using the str.decode() method to convert any bytes objects in the column to UTF-8 encoded strings. This is necessary if the 'comment_text' column contains data that has been encoded as bytes and needs to be converted to strings before being processed further."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HAyMRLGcPKLd"
      },
      "outputs": [],
      "source": [
        "for ds in [ds_train, ds_valid, ds_test]:\n",
        "    ds['comment_text'] = ds['comment_text'].str.decode(\"utf-8\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dlGn_Syx9-x"
      },
      "source": [
        "### Data Expolration and Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NxeTOihX4jjS"
      },
      "outputs": [],
      "source": [
        "print(ds_train.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VVOHo5dgPCSM"
      },
      "outputs": [],
      "source": [
        "ds_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HP6w1eHFPP-4"
      },
      "outputs": [],
      "source": [
        "# Basic Facts for dataset\n",
        "\n",
        "GE_taxonomy = ['approval',\n",
        "               'confusion', 'desire',\n",
        "               'disapproval', 'fear',\n",
        "               'gratitude','joy', 'love', 'neutral',\n",
        "               'optimism', 'pride', 'realization', 'sadness',\n",
        "               'surprise']\n",
        "#Calculating Number of labels\n",
        "num_labels = len(GE_taxonomy)\n",
        "print(f'Total Number or Labels: {num_labels}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Analysis"
      ],
      "metadata": {
        "id": "Y6xTH9n0YK5R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_columns = ds_train.drop('comment_text', axis=1).sum().sort_values(ascending=False)\n",
        "\n",
        "\n",
        "plt.bar(range(len(sorted_columns)), sorted_columns.values)\n",
        "plt.xlabel('Label')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(range(len(sorted_columns)), sorted_columns.index, rotation=90)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uYPcvzsYYPYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see how unequal the dataset is when it comes to distribution frequency.\n",
        "where the pride emotion is almost insignificant."
      ],
      "metadata": {
        "id": "YcTZ83rhnEtQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(sorted_columns)"
      ],
      "metadata": {
        "id": "cMbvPfTfp5Ce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets check the word frequency for each emotion label in the dataset\n"
      ],
      "metadata": {
        "id": "UF00Hc7xiR5s"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjx5MhXjH2J6"
      },
      "source": [
        "# ➤ Experiment Variation 2 \n",
        "## *Run only 1 of the variation cells*\n",
        "\n",
        "As a part of the experiment we will use the NLTK Library to manipulate the content of the corpus with famous preprocessing techniques and observe any differences in order to conclude a hypothesis.\n",
        "\n",
        "#### We will do the following:\n",
        "\n",
        "* Stemming\n",
        "* Lemmetization\n",
        "* Removing of stop words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_DfyMwHTvxO"
      },
      "source": [
        "## ➢ Variation 2 - a\n",
        "In this variation the data is cleaned and preprocessed using NLTK libraries\n",
        "For lemmetization, Stemming, and Stop Words Removal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q3E7MYj6VyAj"
      },
      "outputs": [],
      "source": [
        "# Import Libraries\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Download the vocab\n",
        "nltk.download(\"punkt\")\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "sw_nltk = stopwords.words('english')\n",
        "print(sw_nltk)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ucxtfV4SJa3k"
      },
      "outputs": [],
      "source": [
        "# Building a preprocessing function to clean text\n",
        "def preprocess_corpus_1b(x):\n",
        "\n",
        "  # Adding a space between words and punctation\n",
        "  x = re.sub( r'([a-zA-Z\\[\\]])([,;.!?])', r'\\1 \\2', x)\n",
        "  x = re.sub( r'([,;.!?])([a-zA-Z\\[\\]])', r'\\1 \\2', x)\n",
        "  \n",
        "  # Demojize\n",
        "  x = emoji.demojize(x)\n",
        "  \n",
        "  # Expand contraction\n",
        "  x = contractions.fix(x)\n",
        "  \n",
        "  # Lower\n",
        "  x = x.lower()\n",
        "\n",
        "  # # Handling emojis\n",
        "  x = re.sub(r\"<3\", \" love_heart \", x)\n",
        "  x = re.sub(r\"xd\", \" smiling_face_with_open_mouth_and_tightly_closed_eyes \", x)\n",
        "  x = re.sub(r\":\\)\", \" smiling_face \", x)\n",
        "  x = re.sub(r\"^_^\", \" smiling_face \", x)\n",
        "  x = re.sub(r\"\\*_\\*\", \" star_struck \", x)\n",
        "  x = re.sub(r\":\\(\", \" frowning_face \", x)\n",
        "  x = re.sub(r\":\\^\\(\", \" frowning_face \", x)\n",
        "  x = re.sub(r\";\\(\", \" frowning_face \", x)\n",
        "  x = re.sub(r\":\\/\",  \" confused_face \", x)\n",
        "  x = re.sub(r\";\\)\",  \" wink \", x)\n",
        "  x = re.sub(r\">__<\",  \" unamused \", x)\n",
        "  x = re.sub(r\"\\b([xo]+x*)\\b\", \" xoxo \", x)\n",
        "  x = re.sub(r\"\\b(n+a+h+)\\b\", \" nah \", x)\n",
        "\n",
        "  # Remove special characters and numbers replace by space + remove double space\n",
        "  x = re.sub(r\"\\b([.]{3,})\",\" dots \", x)\n",
        "  x = re.sub(r\"[^A-Za-z!?_]+\",\" \", x)\n",
        "  x = re.sub(r\"\\b([s])\\b *\",\"\", x)\n",
        "  x = re.sub(r\" +\",\" \", x)\n",
        "  x = x.strip()\n",
        "  \n",
        "  \n",
        "  return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SbvWcx0YhY-G"
      },
      "outputs": [],
      "source": [
        "\n",
        "#Lemmatizer using NLTK WordNetLemmatizer\n",
        "def lemmatize_text(text):\n",
        "  s = lemmatizer.lemmatize(text)\n",
        "  return s\n",
        "#Stemming using PorterStemmer\n",
        "def stemming(text):\n",
        "  s = stemmer.stem(text)\n",
        "  return s"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I made multiple columns here so its easier for me to compare each step of the process."
      ],
      "metadata": {
        "id": "Dw7QiOdSKW3Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X0j08DRNUYvj"
      },
      "outputs": [],
      "source": [
        "# Applying the preprocessing function on the dataset\n",
        "\n",
        "datasets = [ds_train, ds_valid, ds_test]\n",
        "for dataset in datasets:\n",
        "  #cleaning of data\n",
        "  dataset[\"wash_text\"] = dataset[\"comment_text\"].apply(preprocess_corpus_1b)\n",
        "  #removal of stop words\n",
        "  dataset[\"prep_text\"] = dataset[\"wash_text\"].apply(lambda x: ' '.join([word for word in x.split() if word not in (sw_nltk)]))\n",
        "  #Lemmatization of words\n",
        "  dataset[\"short_text\"] = dataset[\"prep_text\"].apply(lemmatize_text)\n",
        "  #Stemming of words\n",
        "  dataset[\"clean_text\"] = dataset[\"short_text\"].apply(lemmatize_text)\n",
        "   \n",
        "\n",
        "# Preview of data\n",
        "display(ds_train[['comment_text', 'clean_text']].sample(5))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LuZo0koYnAP"
      },
      "source": [
        "Checking max number of words in a sentences in order to apply padding - \\<pad>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sNOEReSSYjo2"
      },
      "outputs": [],
      "source": [
        "# we check through entire corpus\n",
        "max_length = pd.concat([ds_train['clean_text'], ds_test['clean_text'], ds_valid['clean_text']]).apply(lambda x: len(x.split())).max()\n",
        "print(f'Maximum Length of a sentence after preprocessing: {max_length}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRHuAatsS9Qa"
      },
      "source": [
        "## ➢ Variation 2 - b\n",
        "\n",
        "\n",
        "Where **NO** tokenizations techniques are applied, the data is only cleaned in order to preprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Enp7BF74Pmks"
      },
      "outputs": [],
      "source": [
        "# Building a preprocessing function to clean text\n",
        "def preprocess_corpus_1a(x):\n",
        "\n",
        "  # Adding a space between words and punctation\n",
        "  x = re.sub( r'([a-zA-Z\\[\\]])([,;.!?])', r'\\1 \\2', x)\n",
        "  x = re.sub( r'([,;.!?])([a-zA-Z\\[\\]])', r'\\1 \\2', x)\n",
        "  \n",
        "  # Demojize\n",
        "  x = emoji.demojize(x)\n",
        "  \n",
        "  # Expand contraction\n",
        "  x = contractions.fix(x)\n",
        "  \n",
        "  # Lower\n",
        "  x = x.lower()\n",
        "\n",
        "  # # Handling emojis\n",
        "  x = re.sub(r\"<3\", \" love_heart \", x)\n",
        "  x = re.sub(r\"xd\", \" smiling_face_with_open_mouth_and_tightly_closed_eyes \", x)\n",
        "  x = re.sub(r\":\\)\", \" smiling_face \", x)\n",
        "  x = re.sub(r\"^_^\", \" smiling_face \", x)\n",
        "  x = re.sub(r\"\\*_\\*\", \" star_struck \", x)\n",
        "  x = re.sub(r\":\\(\", \" frowning_face \", x)\n",
        "  x = re.sub(r\":\\^\\(\", \" frowning_face \", x)\n",
        "  x = re.sub(r\";\\(\", \" frowning_face \", x)\n",
        "  x = re.sub(r\":\\/\",  \" confused_face \", x)\n",
        "  x = re.sub(r\";\\)\",  \" wink \", x)\n",
        "  x = re.sub(r\">__<\",  \" unamused \", x)\n",
        "  x = re.sub(r\"\\b([xo]+x*)\\b\", \" xoxo \", x)\n",
        "  x = re.sub(r\"\\b(n+a+h+)\\b\", \" nah \", x)\n",
        "\n",
        "  # Remove special characters and numbers replace by space + remove double space\n",
        "  x = re.sub(r\"\\b([.]{3,})\",\" dots \", x)\n",
        "  x = re.sub(r\"[^A-Za-z!?_]+\",\" \", x)\n",
        "  x = re.sub(r\"\\b([s])\\b *\",\"\", x)\n",
        "  x = re.sub(r\" +\",\" \", x)\n",
        "  x = x.strip()\n",
        "\n",
        "  return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u58ibpgzUaNA"
      },
      "outputs": [],
      "source": [
        "# Applying the preprocessing function on the dataset\n",
        "datasets = [ds_train, ds_valid, ds_test]\n",
        "for dataset in datasets:\n",
        "    dataset[\"clean_text\"] = dataset[\"comment_text\"].apply(preprocess_corpus_1a)\n",
        "\n",
        "\n",
        "# Preview of data\n",
        "display(ds_train[['comment_text', 'clean_text']].sample(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDpVPdtIYA_l"
      },
      "source": [
        "checking max number of words in a sentences in order to apply padding - \\<pad>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Z0E9GydXSf0"
      },
      "outputs": [],
      "source": [
        "max_length = pd.concat([ds_train['clean_text'], ds_test['clean_text'], ds_valid['clean_text']]).apply(lambda x: len(x.split())).max()\n",
        "print(f'Maximum Length of a sentence after cleaning: {max_length}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word Frequency for each label"
      ],
      "metadata": {
        "id": "smWm54tPsucd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "sw_nltk = stopwords.words('english')\n",
        "\n",
        "print(sw_nltk)"
      ],
      "metadata": {
        "id": "I3-5b-rwjjJx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_train2 = ds_train.drop('comment_text', axis = 1)\n",
        "\n",
        "sw_nltk.extend(['!', '?', 'name'])\n",
        "\n",
        "# Count the frequency of each word for each label\n",
        "label_counts = {}\n",
        "for label in ds_train2.columns[1:]:\n",
        "    words2 = [w for w in ' '.join(ds_train2[ds_train2[label] == 1]['clean_text'].values).split() if w not in sw_nltk] # split the text into a list of words\n",
        "    counts = Counter(words2)\n",
        "    label_counts[label] = counts\n",
        "    label_counts[label] = dict(counts.most_common(5))  # keep only the top 5 most frequent words\n",
        "\n",
        "\n",
        "# Plot the word frequency for each label\n",
        "fig, axes = plt.subplots(nrows=4, ncols=4, figsize=(20, 20))\n",
        "for i, label in enumerate(label_counts.keys()):\n",
        "    ax = axes[i//4, i%4]\n",
        "    ax.bar(label_counts[label].keys(), label_counts[label].values())\n",
        "    ax.set_title(label)\n",
        "    ax.tick_params(axis='x', rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ejCvm0DakXVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see what words are the most common for each of the labels. and its quite evident how each words maps so distictly to each emotion respectively"
      ],
      "metadata": {
        "id": "jkWHpCyFrKk3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YuzM7GFoJeqY"
      },
      "source": [
        "## Data Vectorization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcSs2BCLJkPZ"
      },
      "source": [
        "### Creating Train, Validation and Test Variables"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The splits can be automated here to generate multiple train and validate ratios"
      ],
      "metadata": {
        "id": "TPgByQGeK-zo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5l8F15vkRJWr"
      },
      "outputs": [],
      "source": [
        "# Creating train, validation and test variables\n",
        "X_train = ds_train['clean_text']\n",
        "y_train = ds_train.loc[:, GE_taxonomy].values.astype(float)\n",
        "\n",
        "X_valid = ds_valid['clean_text']\n",
        "y_valid = ds_valid.loc[:, GE_taxonomy].values.astype(float)\n",
        "\n",
        "X_test = ds_test['clean_text']\n",
        "y_test = ds_test.loc[:, GE_taxonomy].values.astype(float)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1biOFMlJsRT"
      },
      "source": [
        "### Tokenizing the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use the pretrained [RoBERTa tokenizer](https://huggingface.co/docs/transformers/model_doc/roberta#transformers.RobertaTokenizer)\n",
        "\n",
        "it uses \\<PAD> , \\<s> and \\<UNK> tokens too"
      ],
      "metadata": {
        "id": "Bk1-GmjpLJ_P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is important to notice the use of [max_length](https://colab.research.google.com/drive/1GYb7qo-QCeegdTK5PCNdUy2hSWUSQI93#scrollTo=sNOEReSSYjo2&line=3&uniqifier=1) here in order to pad the sentences well"
      ],
      "metadata": {
        "id": "jPA_OLadLyjC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bs2C0TF3RN5P"
      },
      "outputs": [],
      "source": [
        "# Importing and using pretrained tokenizer\n",
        "\n",
        "from transformers import RobertaTokenizer\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "\n",
        "# Tokenizing train data\n",
        "train_token = tokenizer(\n",
        "    text = X_train.to_list(),\n",
        "    add_special_tokens = True,\n",
        "    max_length = max_length,\n",
        "    truncation = True,\n",
        "    padding = 'max_length', \n",
        "    return_tensors = 'tf',\n",
        "    return_token_type_ids = True,\n",
        "    return_attention_mask = True,\n",
        "    verbose = True)\n",
        "\n",
        "# Tokenizing validation data\n",
        "valid_token = tokenizer(\n",
        "    text = X_valid.to_list(),\n",
        "    add_special_tokens = True,\n",
        "    max_length = max_length,\n",
        "    truncation = True,\n",
        "    padding = 'max_length', \n",
        "    return_tensors = 'tf',\n",
        "    return_token_type_ids = True,\n",
        "    return_attention_mask = True,\n",
        "    verbose = True)\n",
        "\n",
        "# Tokenizing test data\n",
        "test_token = tokenizer(\n",
        "    text = X_test.to_list(),\n",
        "    add_special_tokens = True,\n",
        "    max_length = max_length,\n",
        "    truncation = True,\n",
        "    padding = 'max_length', \n",
        "    return_tensors = 'tf',\n",
        "    return_token_type_ids = True,\n",
        "    return_attention_mask = True,\n",
        "    verbose = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHKRkMfW1mBU"
      },
      "source": [
        "## PreProcessing the Dataset!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDCx4p94GhS8"
      },
      "source": [
        "The following preprocess **only normalizes** the corpus by:\n",
        "\n",
        "* Adding space between punctuations\n",
        "* Removing emojis\n",
        "* Expanding contarctions\n",
        "* Turning the corpus to Lowercase \n",
        "* Removing special characters and numbers replace by space \n",
        "* Removing double spaces\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWXSi3HAiT1J"
      },
      "source": [
        "# Building the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zq95zzQOjIp"
      },
      "source": [
        "### Importing ReBERTa model and config"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We finally import [Roberta Model](https://huggingface.co/docs/transformers/model_doc/roberta) and it's config file"
      ],
      "metadata": {
        "id": "rm6kY4adMP8G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PooTKOAFQZP8"
      },
      "outputs": [],
      "source": [
        "from transformers import RobertaConfig \n",
        "from transformers import TFRobertaModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "duSPu8i8PwKJ"
      },
      "outputs": [],
      "source": [
        "model_name = \"roberta-base\"\n",
        "configuration = RobertaConfig.from_pretrained(model_name, output_hidden_states=False)\n",
        "transformer_model = TFRobertaModel.from_pretrained(model_name, configuration)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sr_00DmGKDSr"
      },
      "source": [
        "# ➤ Experiment Variation '3' ---\n",
        "\n",
        "for this experiment we will finetune and change hyperparameters to compare scoring metrics\n",
        "\n",
        "\n",
        "We will change\n",
        "* Weight Initilizer\n",
        "* Activation Function\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oaweVgRIPU9Z"
      },
      "source": [
        "## ➢ Experiment 3 - a\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Weight Initilizer:\n",
        "> All weight initialized to 0\n",
        "\n",
        "\n",
        "* Activation Function:\n",
        "> Sigmoid"
      ],
      "metadata": {
        "id": "MZIBNC553Xne"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "we use the **Sigmoid** function\n",
        "\\begin{equation}\n",
        "\\sigma(x) = \\frac{1}{1+e^{-x}}\n",
        "\\end{equation}"
      ],
      "metadata": {
        "id": "7Pr9I0NtMvJ8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DxldKrr8QKV_"
      },
      "outputs": [],
      "source": [
        "ACTIVATION = \"sigmoid\"\n",
        "\n",
        "# Initializer that generates tensors initialized to 0.\n",
        "INITIALIZER = tf.keras.initializers.Zeros()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fi-yP2qLPEW6"
      },
      "source": [
        "## ➢ Experiment 3 - b\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Weight Initilizer:\n",
        "> Truncated Normal with Standard Deviation\n",
        "* Activation Function:\n",
        "> Softmax"
      ],
      "metadata": {
        "id": "fza18yrq3hDg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Softmax** Function\n",
        "\n",
        "\\begin{equation}\n",
        "\\mathrm{softmax}(z)i = \\frac{e^{z_i}}{\\sum{j=1}^K e^{z_j}} \\text{ for } i = 1, 2, \\dots, K\n",
        "\\end{equation}\n",
        "\n",
        "where $z = (z_1, z_2, \\dots, z_K)$ are the input values and $\\mathrm{softmax}(z)_i$ represents the $i$-th output of the softmax function."
      ],
      "metadata": {
        "id": "dEx4xUM3NVOV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vvNQJZsuNdAa"
      },
      "outputs": [],
      "source": [
        "ACTIVATION = \"softmax\"\n",
        "INITIALIZER = tf.keras.initializers.TruncatedNormal(stddev=configuration.initializer_range)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZiEfEyLQbCi"
      },
      "source": [
        "## Compiling the model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use Keras to build our Network layers\n",
        "here we also set the number of batches variable.\n"
      ],
      "metadata": {
        "id": "_QT8KyGjNf0W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Activation, Dense, Dropout, InputLayer\n",
        "from keras import layers\n",
        "BATCH = 128"
      ],
      "metadata": {
        "id": "bqiejg_3vkKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also make use of the attention mask since we are padding out sentences."
      ],
      "metadata": {
        "id": "NngHpOl7OW67"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Dropout layer randomly sets input units to 0 with a defined frequency at each step during training time, which helps prevent overfitting. Inputs not set to 0 are scaled up by 1/(1 - rate) such that the sum over all inputs is unchanged."
      ],
      "metadata": {
        "id": "AwrnPGhzOwwz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qgQ22ouwQ3GO"
      },
      "outputs": [],
      "source": [
        "# function for creating RoBERTa based model\n",
        "def create_model(nb_labels):\n",
        "  #nb_labels is the number of labels in our data\n",
        "\n",
        "  # Load the MainLayer\n",
        "  roberta = transformer_model.layers[0]\n",
        "\n",
        "  # Build the model inputs\n",
        "  input_ids = layers.Input(shape=(max_length,), name='input_ids', dtype='int32')\n",
        "  attention_mask = layers.Input(shape=(max_length,), name='attention_mask', dtype='int32')\n",
        "  token_type_ids = layers.Input(shape=(max_length,), name='token_type_ids', dtype='int32')\n",
        "  inputs = {'input_ids': input_ids, 'attention_mask': attention_mask, 'token_type_ids': token_type_ids}\n",
        "\n",
        "  # Load the Transformers RoBERTa model as a layer in a Keras model\n",
        "  roberta_model = roberta(inputs)[1]\n",
        "  dropout = layers.Dropout(configuration.hidden_dropout_prob, name='pooled_output')\n",
        "  pooled_output = dropout(roberta_model, training=False)\n",
        "\n",
        "  emotion = layers.Dense(units=nb_labels, activation=ACTIVATION, kernel_initializer=INITIALIZER, name='emotion')(pooled_output)\n",
        "  outputs = emotion\n",
        "\n",
        "  # And combine it all in a model object\n",
        "  model = tf.keras.models.Model(inputs=inputs, outputs=outputs, name='roBERTa_Label')\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ltFk1LJYRCnF"
      },
      "outputs": [],
      "source": [
        "# Creating a model instance\n",
        "model = create_model(num_labels)\n",
        "\n",
        "# Take a look at the model\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can observe the roberta configuration below and can tune it accordingly"
      ],
      "metadata": {
        "id": "qe6p3Pl5Pr6d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(configuration)"
      ],
      "metadata": {
        "id": "DuKyDmdsPnJ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VYWvg-4LRT7L"
      },
      "outputs": [],
      "source": [
        "# Creating RoBERTa compatible inputs with Input Ids, attention masks and token Ids \n",
        "\n",
        "train = {'input_ids': train_token['input_ids'], 'attention_mask': train_token['attention_mask'],'token_type_ids': train_token['token_type_ids']}\n",
        "val = {'input_ids': valid_token['input_ids'], 'attention_mask': valid_token['attention_mask'],'token_type_ids': valid_token['token_type_ids']}\n",
        "test = {'input_ids': test_token['input_ids'], 'attention_mask': test_token['attention_mask'],'token_type_ids': test_token['token_type_ids']}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9XmSYam6RWky"
      },
      "outputs": [],
      "source": [
        "# Creating TF tensors\n",
        "train_tensor = tf.data.Dataset.from_tensor_slices((train, y_train)).shuffle(len(train)).batch(BATCH)\n",
        "val_tensor = tf.data.Dataset.from_tensor_slices((val, y_valid)).shuffle(len(val)).batch(BATCH)\n",
        "test_tensor = tf.data.Dataset.from_tensor_slices((test, y_test)).shuffle(len(test)).batch(BATCH)\n",
        "print(BATCH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGPT5Lu6OvVT"
      },
      "source": [
        "# ➤ Experiment Variation '4' ---\n",
        "\n",
        "For this experiment we will tuning the:\n",
        "* Loss Function\n",
        "* Number of Epochs\n",
        "* Optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQRiPERFwuYN"
      },
      "source": [
        "## ➢ Experiment Variation 4 - a\n",
        "* Loss:\n",
        "> Custom Binary Cross Entropy \n",
        "* Number of Epochs:\n",
        "> 10\n",
        "* Optimizer:\n",
        "> Adam\n",
        ">>Learning Rate: 1.e-06"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Adam Optimizer**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\\begin{equation}\n",
        "\\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{\\hat{v}_t} + \\epsilon} \\hat{m}_t\n",
        "\\end{equation}\n",
        "\n",
        "and the equations for computing the first and second moments are:\n",
        "\n",
        "\\begin{equation}\n",
        "\\hat{m}_t = \\frac{m_t}{1-\\beta_1^t}, \\quad \\hat{v}_t = \\frac{v_t}{1-\\beta_2^t}\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "O-SZmHVgST7f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The weighted_loss function calculates the binary cross-entropy loss between y_true and y_pred. However, the loss is weighted using the weights parameter. The weights parameter is a 2D array, where the first column contains the weights for negative samples (when y_true is 0) and the second column contains the weights for positive samples (when y_true is 1)."
      ],
      "metadata": {
        "id": "TmNe1CtIQQ3T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7hA1YjOxQ_i"
      },
      "outputs": [],
      "source": [
        "# Number of Epochs\n",
        "EPOCHS = 10\n",
        "\n",
        "# Function for calculating multilabel class weights\n",
        "def calculating_class_weights(y_true):\n",
        "    number_dim = np.shape(y_true)[1]\n",
        "    weights = np.empty([number_dim, 2])\n",
        "    for i in range(number_dim):\n",
        "        weights[i] = compute_class_weight('balanced', classes = [0.,1.], y = y_true[:, i])\n",
        "    return weights\n",
        "\n",
        "class_weights = calculating_class_weights(y_train)\n",
        "\n",
        "# Custom loss function for multilabel\n",
        "\n",
        "def get_weighted_loss(weights):\n",
        "    def weighted_loss(y_true, y_pred):\n",
        "      #first part of mult is appllied to negative samples and second on the positive samples\n",
        "        return K.mean((weights[:,0]**(1-y_true))*(weights[:,1]**(y_true))*K.binary_crossentropy(y_true, y_pred), axis=-1)\n",
        "    return weighted_loss\n",
        "\n",
        "LOSS = get_weighted_loss(class_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49niGDLvxVtL"
      },
      "outputs": [],
      "source": [
        "# Set an optimizer\n",
        "OPTIMIZER = tf.keras.optimizers.experimental.Adam(\n",
        "    learning_rate=5.e-05,\n",
        "    weight_decay=None\n",
        "    )\n",
        "\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer = OPTIMIZER,\n",
        "    loss = LOSS,\n",
        "    metrics=[\"accuracy\"] \n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ty9AfdPXO3PO"
      },
      "source": [
        "## ➢ Experiment Variation 4 - b\n",
        "* Loss:\n",
        "> Custom Binary Cross Entropy \n",
        "* Number of Epochs:\n",
        "> 7\n",
        "* Optimizer:\n",
        "> AdamW\n",
        ">>Learning Rate: 5.e-05\n",
        ">>>Weight Decay: 0.004\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**AdamW Optimizer**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\\begin{equation}\n",
        "\\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{\\hat{v}_t} + \\epsilon} (\\hat{m}_t + \\lambda \\theta_t)\n",
        "\\end{equation}\n",
        "\n",
        "and the equations for computing the first and second moments are:\n",
        "\n",
        "\\begin{equation}\n",
        "\\hat{m}_t = \\frac{m_t}{1-\\beta_1^t}, \\quad \\hat{v}_t = \\frac{v_t}{1-\\beta_2^t}\n",
        "\\end{equation}\n",
        "\n",
        "where $m_t$ and $v_t$ are the first and second moments of the gradients respectively.\n",
        "$\\theta_t$ is the model parameter at time step $t$, $\\eta$ is the learning rate, $\\epsilon$ is a small constant for numerical stability.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "V1pRIza4Rz03"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IA6aM8VB1Kju"
      },
      "source": [
        "Custom loss function to adjust class weights to avoid class imbalance problem"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The weighted_loss function calculates the binary cross-entropy loss between y_true and y_pred. However, the loss is weighted using the weights parameter. The weights parameter is a 2D array, where the first column contains the weights for negative samples (when y_true is 0) and the second column contains the weights for positive samples (when y_true is 1)."
      ],
      "metadata": {
        "id": "Rgq_tJGGQThz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fQtC8OcuRYwR"
      },
      "outputs": [],
      "source": [
        "# Function for calculating multilabel class weights\n",
        "def calculating_class_weights(y_true):\n",
        "    number_dim = np.shape(y_true)[1]\n",
        "    weights = np.empty([number_dim, 2])\n",
        "    for i in range(number_dim):\n",
        "        weights[i] = compute_class_weight('balanced', classes = [0.,1.], y = y_true[:, i])\n",
        "    return weights\n",
        "\n",
        "class_weights = calculating_class_weights(y_train)\n",
        "\n",
        "# Custom loss function for multilabel\n",
        "\n",
        "def get_weighted_loss(weights):\n",
        "    def weighted_loss(y_true, y_pred):\n",
        "      #first part of mult is appllied to negative samples and second on the positive samples\n",
        "        return K.mean((weights[:,0]**(1-y_true))*(weights[:,1]**(y_true))*K.binary_crossentropy(y_true, y_pred), axis=-1)\n",
        "    return weighted_loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-v6f9m8E1djj"
      },
      "source": [
        "Setting Training Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v3oy7m8gRc-U"
      },
      "outputs": [],
      "source": [
        "# Number of Epochs\n",
        "EPOCHS = 8\n",
        "\n",
        "# Set an optimizer\n",
        "OPTIMIZER = tf.keras.optimizers.experimental.AdamW(\n",
        "    learning_rate=5.e-05,\n",
        "    weight_decay = 0.004\n",
        "    )\n",
        "\n",
        "# Set loss\n",
        "LOSS = get_weighted_loss(class_weights)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer = OPTIMIZER,\n",
        "    loss = LOSS,\n",
        "    metrics=[\"accuracy\"] \n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVt4DlFLigzj"
      },
      "source": [
        "# Training the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ITbbFBd53j64"
      },
      "outputs": [],
      "source": [
        "# Summary of the layers of our model\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "segsZEEhByBj"
      },
      "outputs": [],
      "source": [
        "# train the model\n",
        "history = model.fit(train_tensor, \n",
        "                    epochs = EPOCHS,\n",
        "                    validation_data=val_tensor\n",
        "                    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnGYYRZnornS"
      },
      "source": [
        "# Evaluate the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1UjDxwRjR9qy"
      },
      "outputs": [],
      "source": [
        "y_pred_proba = model.predict(test)\n",
        "test"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the group project we will be using argmax function here in order to make the model multi-class and not multi-label."
      ],
      "metadata": {
        "id": "Yb0c9fwwQnoO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2vhPorW_SAJL"
      },
      "outputs": [],
      "source": [
        "\n",
        "# from probabilities to labels using a given threshold\n",
        "def proba_to_labels(y_pred_proba, threshold=0.8):\n",
        "    \n",
        "    y_pred_labels = np.zeros_like(y_pred_proba)\n",
        "    \n",
        "    for i in range(y_pred_proba.shape[0]):\n",
        "        for j in range(y_pred_proba.shape[1]):\n",
        "            if y_pred_proba[i][j] > threshold:\n",
        "                y_pred_labels[i][j] = 1\n",
        "            else:\n",
        "                y_pred_labels[i][j] = 0\n",
        "                \n",
        "    return y_pred_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LbN9kC7nSShR"
      },
      "outputs": [],
      "source": [
        "# Generate labels\n",
        "y_pred_labels = proba_to_labels(y_pred_proba)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sDS5uawlSXkH"
      },
      "outputs": [],
      "source": [
        "# Model evaluation function \n",
        "def model_eval(y_true, y_pred_labels, emotions):\n",
        "    \n",
        "    # Defining variables\n",
        "    precision = []\n",
        "    recall = []\n",
        "    f1 = []\n",
        "    \n",
        "    # Per emotion evaluation      \n",
        "    idx2emotion = {i: e for i, e in enumerate(emotions)}\n",
        "    \n",
        "    for i in range(len(emotions)):\n",
        "   \n",
        "        # Computing precision, recall and f1-score\n",
        "        p, r, f1_score, _ = precision_recall_fscore_support(y_true[:, i], y_pred_labels[:, i], average=\"binary\")\n",
        "        \n",
        "        # Append results in lists\n",
        "        precision.append(round(p, 2))\n",
        "        recall.append(round(r, 2))\n",
        "        f1.append(round(f1_score, 2))\n",
        "    \n",
        "    # Macro evaluation\n",
        "    macro_p, macro_r, macro_f1_score, _ = precision_recall_fscore_support(y_true, y_pred_labels, average=\"macro\")\n",
        "    \n",
        "    # Append results in lists\n",
        "    precision.append(round(macro_p, 2))\n",
        "    recall.append(round(macro_r, 2))\n",
        "    f1.append(round(macro_f1_score, 2))\n",
        "    \n",
        "    # Converting results to a dataframe with gradient\n",
        "    df_results = pd.DataFrame({\"Precision\":precision, \"Recall\":recall, 'F1':f1})\n",
        "    df_results.style.background_gradient(cmap='YlOrRd')\n",
        "    df_results.index = emotions+['MACRO-AVERAGE']\n",
        "    \n",
        "    \n",
        "    return df_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kNrk5sDpPAb"
      },
      "source": [
        "#Results! in F1 score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VBVPoS4ISdo7"
      },
      "outputs": [],
      "source": [
        "seaborn.heatmap(model_eval(y_test, y_pred_labels, GE_taxonomy),annot=True, cmap = 'viridis') \n",
        "#research paper had F1 = 0.46"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\begin{equation}\n",
        "\\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}}\n",
        "\\end{equation}\n",
        "\n",
        "where True Positives are the number of positive instances that are correctly predicted by the model, and False Positives are the number of negative instances that are incorrectly predicted as positive by the model."
      ],
      "metadata": {
        "id": "NYvVnEnBRPvU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\begin{equation}\n",
        "\\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}}\n",
        "\\end{equation}\n",
        "\n",
        "where True Positives are the number of positive instances that are correctly predicted by the model, and False Negatives are the number of positive instances that are incorrectly predicted as negative by the model."
      ],
      "metadata": {
        "id": "YBYfwr-FROQ5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\begin{equation}\n",
        "\\text{F1 score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
        "\\end{equation}\n",
        "\n",
        "The F1 score provides a balance between precision and recall, making it a useful metric when both false positives and false negatives are important considerations."
      ],
      "metadata": {
        "id": "pZE5MB_URU-E"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "ABuvVAxEdtkS",
        "RRKAvaFGdyOZ",
        "4nSvOy40eXKO",
        "X_7zDT_hmQXz",
        "7dlGn_Syx9-x",
        "n_DfyMwHTvxO",
        "sRHuAatsS9Qa",
        "_1biOFMlJsRT",
        "3zq95zzQOjIp",
        "xnGYYRZnornS"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}